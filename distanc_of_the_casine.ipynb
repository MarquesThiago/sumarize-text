{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6e03ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b84d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thiag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bab92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_anime = \"O site oficial da adaptação em anime do mangá de comédia romântica Aharen-san wa Hakarenai divulgou o segundo vídeo promocional do mesmo.\\nO vídeo anuncia mais três novos dubladores, apresenta a música tema de abertura do grupo TrySail “Hanarenai Kyori” (Inseparable Distance) e revela a estreia do anime para o dia 1º de abril.\\nSinopse\\nReina Aharen, uma aluna pequena e bonita com uma voz baixa, é muito ruim em determinar distâncias e espaço pessoal. Às vezes ela está a centímetros de seu rosto e, outras vezes, a quilômetros de distância! O único que tenta entender suas travessuras é Raidou Matsuboshi, que está sentado ao lado dela na classe. Ele tem um rosto ameaçador, mas na realidade, ele é um menino gentil com uma imaginação que às vezes pode correr solta. Aharen-san wa Hakarenai segue o estranho par enquanto sua estranha amizade começa a florescer quando Raidou pega a borracha de Aharen para ela. Ela interpreta mal o gesto dele e agora acredita que eles são os melhores amigos, mostrando como as coisas mais simples podem ser os mais complicados dos desafios para eles.\\nAharen-san wa Hakarenai estreará portanto no bloco Animeism nos canais TBS e MBS no dia 1 de abril.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5ff255",
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrds = \"a, agora, ainda, alguém, algum, alguma, algumas, alguns, ampla, amplas, amplo, amplos, ante, antes, ao, aos, após, aquela, aquelas, aquele, aqueles, aquilo, as, até, através, cada, coisa, coisas, com, como, contra, contudo, da, daquele, daqueles, das, de, dela, delas, dele, deles, depois, dessa, dessas, desse, desses, desta, destas, deste, deste, destes, deve, devem, devendo, dever, deverá, deverão, deveria, deveriam, devia, deviam, disse, disso, disto, dito, diz, dizem, do, dos, e, é, ela, elas, ele, eles, em, enquanto, entre, era, essa, essas, esse, esses, esta, está, estamos, estão, estas, estava, estavam, estávamos, este, estes, estou, eu, fazendo, fazer, feita, feitas, feito, feitos, foi, for, foram, fosse, fossem, grande, grandes, há, isso, isto, já, la, lá, lhe, lhes, lo, mas, me, mesma, mesmas, mesmo, mesmos, meu, meus, minha, minhas, muita, muitas, muito, muitos, na, não, nas, nem, nenhum, nessa, nessas, nesta, nestas, ninguém, no, nos, nós, nossa, nossas, nosso, nossos, num, numa, nunca, o, os, ou, outra, outras, outro, outros, para, pela, pelas, pelo, pelos, pequena, pequenas, pequeno, pequenos, per, perante, pode, pude, podendo, poder, poderia, poderiam, podia, podiam, pois, por, porém, porque, posso, pouca, poucas, pouco, poucos, primeiro, primeiros, própria, próprias, próprio, próprios, quais, qual, quando, quanto, quantos, que, quem, são, se, seja, sejam, sem, sempre, sendo, será, serão, seu, seus, si, sido, só, sob, sobre, sua, suas, talvez, também, tampouco, te, tem, tendo, tenha, ter, teu, teus, ti, tido, tinha, tinham, toda, todas, todavia, todo, todos, tu, tua, tuas, tudo, última, últimas, último, últimos, um, uma, umas, uns, vendo, ver, vez, vindo, vir, vos, vós\"\n",
    "stop_word = [n.strip() for n  in stpwrds.split(',')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "157d2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_clear(text):\n",
    "    \n",
    "    ''' \n",
    "    simples clear, elimined space and break lines \n",
    "    :param text: str text\n",
    "    return: str re.sub(r'\\s+',\" \", text.lower())\n",
    "    '''\n",
    "    \n",
    "    return re.sub(r'\\s+',\" \", text.lower())\n",
    "\n",
    "def normalize_text(text):\n",
    "    \n",
    "    ''' \n",
    "    harmonizetion of text,separede text in words, \n",
    "    remove space, simbols and stop word (words not contribuinls to meaning) \n",
    "    :param text: str text\n",
    "    return: list text_harmonization: list fo words harmonizeds \n",
    "    '''\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text.lower(), language=\"Portuguese\")\n",
    "    text_harmonization = [word for word  in tokens if word not in stop_word and word not in (string.punctuation + \"“”\") and not word.isdigit()]  \n",
    "    return text_harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "479dc004",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sent_tokenize() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentency \u001b[38;5;241m=\u001b[39m [sentency \u001b[38;5;28;01mfor\u001b[39;00m sentency \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m      2\u001b[0m normalize_sent \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(normalize_text(sent)) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentency]\n",
      "\u001b[1;31mTypeError\u001b[0m: sent_tokenize() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "sentency = [sentency for sentency in nltk.sent_tokenize()]\n",
    "normalize_sent = [ \" \".join(normalize_text(sent)) for sent in sentency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e3f0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables(text):\n",
    "    \n",
    "    '''  \n",
    "    create aariables for use in calculate of the lunh, starting by text and number of ranking \n",
    "    created variables in function \n",
    "    tokens: separed text in sentences, setence_clear: applied cleaning in sentences (remove break line,stop words and others), \n",
    "    all_words list of the all words in text (aren't stop words) and rank:  n words, most used in text  \n",
    "    :param text: str \n",
    "    :param rank: int -> number to return of words most commons in text\n",
    "    :return dict {\"tokens\": tokens, \"sentence_clear\": clear_sentency, \"all_words\": words, \"rank\": rank_words}'''\n",
    "    \n",
    "    tokens= nltk.sent_tokenize(text.lower(), language = \"Portuguese\")\n",
    "    clear_sentency = [\" \".join(normalize_text(sentency)) for sentency in tokens]\n",
    "    words = [normalize_text(sentency) for sentency in tokens]\n",
    "    \n",
    "    return  {\"tokens\": tokens, \"sentence_clear\": clear_sentency, \"all_words\": words,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fb57be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = variables(art_anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49b6c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(text_one, text_two):\n",
    "    \n",
    "    '''\n",
    "    return a list of the words uniques in two texts \n",
    "    :param text_one : str -> first text \n",
    "    :param text_teo : str -> second text\n",
    "    >return list list(set(var[\"all_words\"][0] + var[\"all_words\"][1])):\n",
    "    '''\n",
    "    return list(set(text_one + text_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c55f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_unique_word(text_one, text_two):\n",
    "    words  = unique_words(text_one, text_two)\n",
    "    \n",
    "    unique_text_one = [len(re.findall(word, \" \".join(text_one) )) for word in words]\n",
    "    unique_text_two = [len(re.findall(word, \" \".join(text_two) )) for word in words]\n",
    "    \n",
    "    return {\"nunique\": [unique_text_one, unique_text_two], \"labels\" : words }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "91a4d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_nunique(vector):\n",
    "    return [float(value / len(vector)) for value in vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da92ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique = num_unique_word(var[\"all_words\"][0] , var[\"all_words\"][1])\n",
    "norm_nunq = [normalize_nunique(sent) for sent in num_unique[\"nunique\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "91e762af",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nuniques = pd.DataFrame(data = norm_nunq, columns = num_unique[\"labels\"])\n",
    "words_uniques = pd.DataFrame(data = num_unique[\"nunique\"], columns = num_unique[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4a5cb761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comédia</th>\n",
       "      <th>wa</th>\n",
       "      <th>trysail</th>\n",
       "      <th>três</th>\n",
       "      <th>grupo</th>\n",
       "      <th>site</th>\n",
       "      <th>adaptação</th>\n",
       "      <th>1º</th>\n",
       "      <th>hakarenai</th>\n",
       "      <th>distance</th>\n",
       "      <th>...</th>\n",
       "      <th>vídeo</th>\n",
       "      <th>hanarenai</th>\n",
       "      <th>oficial</th>\n",
       "      <th>abertura</th>\n",
       "      <th>divulgou</th>\n",
       "      <th>estreia</th>\n",
       "      <th>música</th>\n",
       "      <th>abril</th>\n",
       "      <th>novos</th>\n",
       "      <th>revela</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comédia  wa  trysail  três  grupo  site  adaptação  1º  hakarenai  \\\n",
       "0        1   1        0     0      0     1          1   0          1   \n",
       "1        0   0        1     1      1     0          0   1          0   \n",
       "\n",
       "   distance  ...  vídeo  hanarenai  oficial  abertura  divulgou  estreia  \\\n",
       "0         0  ...      1          0        1         0         1        0   \n",
       "1         1  ...      1          1        0         1         0        1   \n",
       "\n",
       "   música  abril  novos  revela  \n",
       "0       0      0      0       0  \n",
       "1       1      1      1       1  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8af32112",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = [words_uniques[col].sum() for col in list(words_uniques.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "067e97a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iDF(data):\n",
    "    count_in_sent = [data[col].apply(\n",
    "                    lambda x: 1 if x > 0 else 0).sum() for col in list(data.columns) ]\n",
    "    \n",
    "    return  [1.0 + log(data.shape[0])/ sent for sent in count_in_sent if sent > 0 and not sent is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9821428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = calculate_iDF(words_uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "519914ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_if_IDF(data, idf):\n",
    "    columns = list(data.columns)\n",
    "    data_process = data.copy()\n",
    "    for index in range(len(columns)):\n",
    "        data_process[columns[index]] = data[columns[index]].apply(lambda x: x * idf[index])\n",
    "    return data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d2c8815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = product_if_IDF(words_nuniques, IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ca971c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_TRAN = TF_IDF.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8fa1400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_TRAN = TF_IDF_TRAN.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cb6cb1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.933786212659348"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TF_IDF_TRAN[0] * IDF).sum() + (TF_IDF_TRAN[1] * IDF).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9639ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cd7c1c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008532647236123963"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((TF_IDF_TRAN[0] * IDF) * (TF_IDF_TRAN[1] * IDF)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17010cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c6f1b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_doc = (TF_IDF_TRAN[1] * IDF).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2369b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = (TF_IDF_TRAN[1].apply(lambda x: x**2).sum())**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cc271607",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = (sum([i**2 for  i in IDF]))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5d1f0f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7970138871545912"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_doc / (query * doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1435c574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.834855435231046"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_distance( num_unique[\"nunique\"][0], num_unique[\"nunique\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
